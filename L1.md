* Analysis of algorithms:

  * Theoretical study of computer program performance and resource usage. (rusage is the communication and memory req)
  * Things as important as performance-
    * correctness
    * modularity
    * robustness
    * maintainability
    * functionality
    * user friendliness\
    \* Above aspects are aided if performance of algorithm is good

*  Running time depends on input size and kind of input permutation
  * Usually interested in worst case
  * Sometimes in average case (amortized algorithms). Expected running time.
      * Requires some assumption of distribution of input
        * Most common assumption- Uniform distribution. Each case of input equally likely to happen
  * Asymptotic analysis
    * Look at growth of T(n) as n-> inf
    * theta notation
    * $ax^2+bx+c=\theta(x^2)$ (generalize for higher order polynomials)
    * constants c, and functions g(x), f(x) are assumed > 0 in all asymptotic notations
* Sorting
  * Insertion sort
    * Pseudo code CLRS Pg 26
    * Demonstrate with toy example
    * Algorithm invariant
    * Inplace. Constant memory
    * $\theta(n)$ if input already sorted
    * $\theta(n*2)$ if input is in reverse order
    * Since time complexity is of different orders in best and worst case, hence we cannot say a single $\theta$ for all cases of insertion sort. Can only say $ O(n^2) $ that covers all cases (that is the worst case)
    * Insertion sort is moderately fast for small n and very slow for large n
  * Merge sort
    * Key is the merge subroutine. Time= $\theta (n)$
    * Merge sort recurrence relation. Tree recurrence.
    * Leaves in tree = $2^{height-1}$ = n if every node branches into 2 nodes (a=2)
    * Running time is $\theta(n*\log n)$ for all cases (best or worst examples)
  * In pratice, merge sort beats insertion sort for n>=30.
